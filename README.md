# Machine-Translation-Using-Attention-Mechanism

* To implement an Encoder and Decoder architecture with attention using three different scoring function
<a href="https://arxiv.org/pdf/1508.04025.pdf">https://arxiv.org/pdf/1508.04025.pdf</a>
<a href="https://arxiv.org/pdf/1409.0473.pdf">https://arxiv.org/pdf/1409.0473.pdf</a>
* In Global attention, there are 3 types of scoring functions.
 I have created 3 models for each scoring function**
<img src='https://i.imgur.com/iD2jZo3.png'>

    * In model 1 implemnted "dot" score function
    * In model 2 implemnted "general" score function
    * In model 3 implemnted "concat" score function.<br>

* Resources:
    a. <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Resource 1</a>
    b. <a href="https://www.tensorflow.org/tutorials/text/nmt_with_attention">Resource 2</a>
    c. <a href="https://stackoverflow.com/questions/44238154/what-is-the-difference-between-luong-attention-and-bahdanau-attention#:~:text=Luong%20attention%20used%20top%20hidden,hidden%20state%20at%20time%20t.">Resource 3</a>
